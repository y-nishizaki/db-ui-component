# ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Databricks UI Component Libraryã‚’æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ¯ æ¦‚è¦

æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã§ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€å¯ç”¨æ€§ã‚’è€ƒæ…®ã—ãŸé©åˆ‡ãªè¨­å®šãŒå¿…è¦ã§ã™ã€‚

## ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥

### 1. ç’°å¢ƒåˆ¥ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```python
# ç’°å¢ƒè¨­å®šã‚¯ãƒ©ã‚¹
class EnvironmentConfig:
    def __init__(self, environment):
        self.environment = environment
        self.configs = {
            'development': {
                'debug': True,
                'log_level': 'DEBUG',
                'cache_enabled': False,
                'max_data_size': 10000
            },
            'staging': {
                'debug': False,
                'log_level': 'INFO',
                'cache_enabled': True,
                'max_data_size': 50000
            },
            'production': {
                'debug': False,
                'log_level': 'WARNING',
                'cache_enabled': True,
                'max_data_size': 100000,
                'rate_limiting': True,
                'ssl_required': True
            }
        }
    
    def get_config(self):
        """ç’°å¢ƒè¨­å®šã‚’å–å¾—"""
        return self.configs.get(self.environment, self.configs['development'])
    
    def is_production(self):
        """æœ¬ç•ªç’°å¢ƒã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return self.environment == 'production'

# ä½¿ç”¨ä¾‹
config = EnvironmentConfig('production')
settings = config.get_config()
print(f"æœ¬ç•ªç’°å¢ƒè¨­å®š: {settings}")
```

### 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†

```python
import yaml
import os

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†
class ConfigManager:
    def __init__(self, config_path='config.yaml'):
        self.config_path = config_path
        self.config = self.load_config()
    
    def load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r') as file:
                return yaml.safe_load(file)
        else:
            return self.get_default_config()
    
    def get_default_config(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—"""
        return {
            'database': {
                'host': 'localhost',
                'port': 5432,
                'name': 'dashboard_db',
                'user': 'dashboard_user'
            },
            'security': {
                'ssl_required': True,
                'session_timeout': 3600,
                'max_login_attempts': 5
            },
            'performance': {
                'cache_enabled': True,
                'cache_ttl': 300,
                'max_data_size': 100000
            },
            'logging': {
                'level': 'INFO',
                'file': 'dashboard.log',
                'max_size': '10MB',
                'backup_count': 5
            }
        }
    
    def get_database_config(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’å–å¾—"""
        return self.config.get('database', {})
    
    def get_security_config(self):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã‚’å–å¾—"""
        return self.config.get('security', {})
    
    def get_performance_config(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šã‚’å–å¾—"""
        return self.config.get('performance', {})

# ä½¿ç”¨ä¾‹
config_manager = ConfigManager()
db_config = config_manager.get_database_config()
security_config = config_manager.get_security_config()
```

## ğŸ³ Docker ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### 1. Dockerfile ã®ä½œæˆ

```dockerfile
# Dockerfile
FROM python:3.9-slim

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
WORKDIR /app

# ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Pythonä¾å­˜é–¢ä¿‚ã®ã‚³ãƒ”ãƒ¼
COPY requirements.txt .

# Pythonä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ”ãƒ¼
COPY . .

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
ENV PYTHONPATH=/app
ENV FLASK_ENV=production
ENV FLASK_APP=app.py

# ãƒãƒ¼ãƒˆã®å…¬é–‹
EXPOSE 5000

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "app:app"]
```

### 2. Docker Compose ã®è¨­å®š

```yaml
# docker-compose.yml
version: '3.8'

services:
  dashboard-app:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://user:password@db:5432/dashboard
      - REDIS_URL=redis://redis:6379
      - SECRET_KEY=${SECRET_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped

  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=dashboard
      - POSTGRES_USER=dashboard_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - dashboard-app

volumes:
  postgres_data:
  redis_data:
```

### 3. Nginx è¨­å®š

```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream dashboard_app {
        server dashboard-app:5000;
    }

    server {
        listen 80;
        server_name your-domain.com;
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name your-domain.com;

        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";

        location / {
            proxy_pass http://dashboard_app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã®é…ä¿¡
        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

## â˜ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### 1. AWS ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```python
# AWS ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè¨­å®š
import boto3
import json

class AWSDeployment:
    def __init__(self, region='us-east-1'):
        self.region = region
        self.ec2 = boto3.client('ec2', region_name=region)
        self.ecs = boto3.client('ecs', region_name=region)
        self.elbv2 = boto3.client('elbv2', region_name=region)
    
    def create_ecs_cluster(self, cluster_name):
        """ECSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ä½œæˆ"""
        try:
            response = self.ecs.create_cluster(
                clusterName=cluster_name,
                capacityProviders=['FARGATE'],
                defaultCapacityProviderStrategy=[
                    {
                        'capacityProvider': 'FARGATE',
                        'weight': 1
                    }
                ]
            )
            return response['cluster']
        except Exception as e:
            print(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_task_definition(self, family, image_uri):
        """ã‚¿ã‚¹ã‚¯å®šç¾©ã®ä½œæˆ"""
        task_definition = {
            'family': family,
            'networkMode': 'awsvpc',
            'requiresCompatibilities': ['FARGATE'],
            'cpu': '256',
            'memory': '512',
            'executionRoleArn': 'arn:aws:iam::account:role/ecsTaskExecutionRole',
            'containerDefinitions': [
                {
                    'name': 'dashboard-app',
                    'image': image_uri,
                    'portMappings': [
                        {
                            'containerPort': 5000,
                            'protocol': 'tcp'
                        }
                    ],
                    'environment': [
                        {
                            'name': 'DATABASE_URL',
                            'value': 'postgresql://user:password@db:5432/dashboard'
                        }
                    ],
                    'logConfiguration': {
                        'logDriver': 'awslogs',
                        'options': {
                            'awslogs-group': '/ecs/dashboard-app',
                            'awslogs-region': self.region,
                            'awslogs-stream-prefix': 'ecs'
                        }
                    }
                }
            ]
        }
        
        try:
            response = self.ecs.register_task_definition(**task_definition)
            return response['taskDefinition']
        except Exception as e:
            print(f"ã‚¿ã‚¹ã‚¯å®šç¾©ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

# ä½¿ç”¨ä¾‹
aws_deployment = AWSDeployment('us-east-1')
cluster = aws_deployment.create_ecs_cluster('dashboard-cluster')
task_def = aws_deployment.create_task_definition('dashboard-task', 'your-registry/dashboard-app:latest')
```

### 2. Azure ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

```python
# Azure ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè¨­å®š
from azure.identity import DefaultAzureCredential
from azure.mgmt.containerinstance import ContainerInstanceManagementClient
from azure.mgmt.containerinstance.models import ContainerGroup, Container, ResourceRequests, ResourceRequirements

class AzureDeployment:
    def __init__(self, subscription_id, resource_group):
        self.subscription_id = subscription_id
        self.resource_group = resource_group
        self.credential = DefaultAzureCredential()
        self.container_client = ContainerInstanceManagementClient(
            credential=self.credential,
            subscription_id=subscription_id
        )
    
    def create_container_group(self, name, image, location='eastus'):
        """ã‚³ãƒ³ãƒ†ãƒŠã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ"""
        container_group = ContainerGroup(
            location=location,
            containers=[
                Container(
                    name='dashboard-app',
                    image=image,
                    resources=ResourceRequirements(
                        requests=ResourceRequests(
                            memory_in_gb=1.0,
                            cpu=1.0
                        )
                    ),
                    ports=[{'port': 5000}]
                )
            ],
            os_type='Linux',
            ip_address={
                'type': 'Public',
                'ports': [
                    {
                        'protocol': 'TCP',
                        'port': 5000
                    }
                ]
            }
        )
        
        try:
            poller = self.container_client.container_groups.begin_create_or_update(
                resource_group_name=self.resource_group,
                container_group_name=name,
                container_group=container_group
            )
            result = poller.result()
            return result
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒŠã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

# ä½¿ç”¨ä¾‹
azure_deployment = AzureDeployment('subscription-id', 'resource-group')
container_group = azure_deployment.create_container_group(
    'dashboard-app',
    'your-registry.azurecr.io/dashboard-app:latest'
)
```

## ğŸ”„ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 1. GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy Dashboard App

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run tests
      run: |
        pytest tests/
    
    - name: Run linting
      run: |
        flake8 db_ui_components/
        black --check db_ui_components/

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1
    
    - name: Login to Docker Hub
      uses: docker/login-action@v1
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v2
      with:
        context: .
        push: true
        tags: |
          your-username/dashboard-app:latest
          your-username/dashboard-app:${{ github.sha }}
    
    - name: Deploy to production
      run: |
        # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
        ./deploy.sh production

  deploy-staging:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    steps:
    - uses: actions/checkout@v2
    
    - name: Deploy to staging
      run: |
        ./deploy.sh staging
```

### 2. Jenkins Pipeline

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        DOCKER_IMAGE = 'dashboard-app'
        DOCKER_TAG = "${env.BUILD_NUMBER}"
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Test') {
            steps {
                sh 'pip install -r requirements.txt'
                sh 'pip install -r requirements-dev.txt'
                sh 'pytest tests/'
                sh 'flake8 db_ui_components/'
                sh 'black --check db_ui_components/'
            }
        }
        
        stage('Build') {
            steps {
                script {
                    docker.build("${DOCKER_IMAGE}:${DOCKER_TAG}")
                }
            }
        }
        
        stage('Push') {
            steps {
                script {
                    docker.withRegistry('https://registry.hub.docker.com', 'docker-hub-credentials') {
                        docker.image("${DOCKER_IMAGE}:${DOCKER_TAG}").push()
                        docker.image("${DOCKER_IMAGE}:${DOCKER_TAG}").push('latest')
                    }
                }
            }
        }
        
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                sh './deploy.sh production'
            }
        }
    }
    
    post {
        always {
            cleanWs()
        }
        success {
            echo 'Pipeline succeeded!'
        }
        failure {
            echo 'Pipeline failed!'
        }
    }
}
```

## ğŸ“Š ç›£è¦–ã¨ãƒ­ã‚°

### 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦–

```python
# ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
import time
import psutil
import logging
from prometheus_client import Counter, Histogram, Gauge

class MonitoringSystem:
    def __init__(self):
        self.request_counter = Counter('http_requests_total', 'Total HTTP requests')
        self.request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration')
        self.memory_usage = Gauge('memory_usage_bytes', 'Memory usage in bytes')
        self.cpu_usage = Gauge('cpu_usage_percent', 'CPU usage percentage')
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('app.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def monitor_request(self, func):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def wrapper(*args, **kwargs):
            start_time = time.time()
            self.request_counter.inc()
            
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                self.request_duration.observe(duration)
                self.logger.info(f"Request completed in {duration:.2f}s")
                return result
            except Exception as e:
                self.logger.error(f"Request failed: {e}")
                raise
        
        return wrapper
    
    def update_system_metrics(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°"""
        memory = psutil.virtual_memory()
        cpu = psutil.cpu_percent()
        
        self.memory_usage.set(memory.used)
        self.cpu_usage.set(cpu)
        
        self.logger.info(f"Memory: {memory.percent}%, CPU: {cpu}%")

# ä½¿ç”¨ä¾‹
monitoring = MonitoringSystem()

@monitoring.monitor_request
def render_dashboard(data):
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
    return dashboard.render()

# å®šæœŸçš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
import threading
import time

def update_metrics():
    while True:
        monitoring.update_system_metrics()
        time.sleep(60)  # 1åˆ†é–“éš”

metrics_thread = threading.Thread(target=update_metrics, daemon=True)
metrics_thread.start()
```

### 2. ãƒ­ã‚°é›†ç´„

```python
# ãƒ­ã‚°é›†ç´„ã‚·ã‚¹ãƒ†ãƒ 
import json
from datetime import datetime

class LogAggregator:
    def __init__(self, log_file='aggregated.log'):
        self.log_file = log_file
    
    def log_event(self, event_type, data, level='INFO'):
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ã‚’è¨˜éŒ²"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level,
            'event_type': event_type,
            'data': data
        }
        
        with open(self.log_file, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
    
    def get_recent_logs(self, hours=24):
        """æœ€è¿‘ã®ãƒ­ã‚°ã‚’å–å¾—"""
        logs = []
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        with open(self.log_file, 'r') as f:
            for line in f:
                try:
                    log_entry = json.loads(line.strip())
                    log_time = datetime.fromisoformat(log_entry['timestamp'])
                    if log_time >= cutoff_time:
                        logs.append(log_entry)
                except json.JSONDecodeError:
                    continue
        
        return logs

# ä½¿ç”¨ä¾‹
log_aggregator = LogAggregator()

# ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ã®è¨˜éŒ²
log_aggregator.log_event('dashboard_rendered', {
    'user_id': 'user1',
    'chart_count': 5,
    'render_time': 2.5
})

# æœ€è¿‘ã®ãƒ­ã‚°ã‚’å–å¾—
recent_logs = log_aggregator.get_recent_logs(1)  # éå»1æ™‚é–“
```

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](./performance.md) - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–
- [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£](./security.md) - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](../troubleshooting/faq.md) - ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–¢é€£ã®FAQ

## â“ ã‚µãƒãƒ¼ãƒˆ

ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã§å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

- [ã‚ˆãã‚ã‚‹å•é¡Œ](../troubleshooting/faq.md)
- [ã‚¨ãƒ©ãƒ¼ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](../troubleshooting/errors.md)
- [GitHub Issues](https://github.com/databricks/db-ui-components/issues)