# Databricksã§ã®ä½¿ç”¨ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Databricksç’°å¢ƒã§Databricks UI Component Libraryã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚

## ğŸš€ Databricksç’°å¢ƒã§ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

#### Databricks Runtime 10.4ä»¥ä¸Š

```python
# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®æœ€åˆã®ã‚»ãƒ«ã§å®Ÿè¡Œ
!pip install db-ui-components

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
import db_ui_components
print(f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {db_ui_components.__version__}")
```

#### Databricks Runtime 10.4æœªæº€

```python
# ä¾å­˜é–¢ä¿‚ã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install pandas>=1.5.0
!pip install numpy>=1.21.0
!pip install plotly>=5.0.0
!pip install dash>=2.0.0
!pip install db-ui-components
```

### 2. åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹

```python
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
from db_ui_components import ChartComponent, TableComponent, Dashboard

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
np.random.seed(42)
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=30, freq='D'),
    'sales': np.random.normal(1000, 200, 30),
    'profit': np.random.normal(200, 50, 30)
})

# ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä½œæˆ
chart = ChartComponent(
    data=df,
    chart_type='line',
    x_column='date',
    y_column='sales',
    title='å£²ä¸Šæ¨ç§»'
)

# Databricksã§è¡¨ç¤º
displayHTML(chart.render())
```

## ğŸ“Š Databricksã§ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†

### 1. Delta Lakeã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

```python
# Deltaãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
df = spark.read.format("delta").load("/path/to/your/table").toPandas()

# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values('date')

# ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§è¡¨ç¤º
chart = ChartComponent(
    data=df,
    chart_type='line',
    x_column='date',
    y_column='sales',
    title='Delta Lakeã‹ã‚‰ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿'
)

displayHTML(chart.render())
```

### 2. Spark DataFrameã®å¤‰æ›

```python
# Spark DataFrameã‚’Pandas DataFrameã«å¤‰æ›
spark_df = spark.sql("SELECT * FROM sales_table WHERE date >= '2024-01-01'")
df = spark_df.toPandas()

# ãƒ‡ãƒ¼ã‚¿ã®é›†ç´„
daily_sales = df.groupby('date')['sales'].sum().reset_index()

# ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
chart = ChartComponent(
    data=daily_sales,
    chart_type='bar',
    x_column='date',
    y_column='sales',
    title='æ—¥æ¬¡å£²ä¸Šé›†è¨ˆ'
)

displayHTML(chart.render())
```

### 3. å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†

```python
# å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
large_df = spark.read.format("delta").load("/path/to/large/table")

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ã‹ã‚‰Pandasã«å¤‰æ›
sampled_df = large_df.sample(fraction=0.1, seed=42).toPandas()

# ã¾ãŸã¯ã€ç‰¹å®šã®æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å–å¾—
recent_df = large_df.filter("date >= '2024-01-01'").toPandas()

# ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
chart = ChartComponent(
    data=sampled_df,
    chart_type='line',
    x_column='date',
    y_column='sales',
    title='å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœ'
)

displayHTML(chart.render())
```

## ğŸ›ï¸ Databricksã§ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ

### 1. åŸºæœ¬çš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```python
from db_ui_components import Dashboard, ChartComponent, TableComponent

# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
sales_data = spark.sql("""
    SELECT 
        date,
        SUM(sales) as total_sales,
        SUM(profit) as total_profit,
        COUNT(*) as transaction_count
    FROM sales_table 
    WHERE date >= '2024-01-01'
    GROUP BY date
    ORDER BY date
""").toPandas()

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ
dashboard = Dashboard(title='Databrickså£²ä¸Šãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰')

# å£²ä¸Šæ¨ç§»ã‚°ãƒ©ãƒ•
sales_chart = ChartComponent(
    data=sales_data,
    chart_type='line',
    x_column='date',
    y_column='total_sales',
    title='å£²ä¸Šæ¨ç§»'
)
dashboard.add_component(sales_chart, position=(0, 0))

# åˆ©ç›Šæ¨ç§»ã‚°ãƒ©ãƒ•
profit_chart = ChartComponent(
    data=sales_data,
    chart_type='line',
    x_column='date',
    y_column='total_profit',
    title='åˆ©ç›Šæ¨ç§»'
)
dashboard.add_component(profit_chart, position=(0, 1))

# è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
detail_table = TableComponent(
    data=sales_data,
    enable_csv_download=True,
    sortable=True,
    searchable=True,
    title='å£²ä¸Šè©³ç´°'
)
dashboard.add_component(detail_table, position=(1, 0))

# è¡¨ç¤º
displayHTML(dashboard.render())
```

### 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿æ›´æ–°

```python
# å®šæœŸçš„ãªãƒ‡ãƒ¼ã‚¿æ›´æ–°
import time
from datetime import datetime

def update_dashboard():
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’å®šæœŸçš„ã«æ›´æ–°"""
    while True:
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        latest_data = spark.sql("""
            SELECT 
                date,
                SUM(sales) as total_sales
            FROM sales_table 
            WHERE date >= DATE_SUB(CURRENT_DATE(), 30)
            GROUP BY date
            ORDER BY date
        """).toPandas()
        
        # ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°
        chart = ChartComponent(
            data=latest_data,
            chart_type='line',
            x_column='date',
            y_column='total_sales',
            title=f'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å£²ä¸Šæ¨ç§» (æ›´æ–°: {datetime.now().strftime("%H:%M:%S")})'
        )
        
        # è¡¨ç¤º
        displayHTML(chart.render())
        
        # 5åˆ†å¾…æ©Ÿ
        time.sleep(300)

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œï¼ˆæ³¨æ„: å®Ÿéš›ã®ä½¿ç”¨ã§ã¯é©åˆ‡ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼‰
# update_dashboard()
```

## ğŸ” Databricksã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½

### 1. å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

```python
from db_ui_components import FilterComponent

# åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
categories = spark.sql("SELECT DISTINCT category FROM sales_table").toPandas()['category'].tolist()

# ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
category_filter = FilterComponent(
    filter_type='dropdown',
    column='category',
    options=categories,
    placeholder='ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ',
    title='ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼'
)

displayHTML(category_filter.render())

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã‚’è¡¨ç¤º
def show_filtered_data(selected_category):
    """é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"""
    filtered_data = spark.sql(f"""
        SELECT * FROM sales_table 
        WHERE category = '{selected_category}'
        ORDER BY date
    """).toPandas()
    
    chart = ChartComponent(
        data=filtered_data,
        chart_type='line',
        x_column='date',
        y_column='sales',
        title=f'{selected_category}ã‚«ãƒ†ã‚´ãƒªã®å£²ä¸Š'
    )
    
    displayHTML(chart.render())
```

### 2. æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

```python
# æ—¥ä»˜ç¯„å›²ã®å–å¾—
date_range = spark.sql("""
    SELECT 
        MIN(date) as min_date,
        MAX(date) as max_date
    FROM sales_table
""").toPandas()

# æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
date_filter = FilterComponent(
    filter_type='date',
    column='date',
    start_date=date_range['min_date'].iloc[0],
    end_date=date_range['max_date'].iloc[0],
    title='æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼'
)

displayHTML(date_filter.render())
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥

```python
# é »ç¹ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
frequently_used_data = spark.sql("""
    SELECT 
        date,
        category,
        region,
        SUM(sales) as total_sales,
        SUM(profit) as total_profit
    FROM sales_table 
    WHERE date >= '2024-01-01'
    GROUP BY date, category, region
""").cache()

# Pandas DataFrameã«å¤‰æ›
df = frequently_used_data.toPandas()

# ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
chart = ChartComponent(
    data=df,
    chart_type='line',
    x_column='date',
    y_column='total_sales',
    title='ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå£²ä¸Šãƒ‡ãƒ¼ã‚¿'
)

displayHTML(chart.render())
```

### 2. ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
# å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
def get_sampled_data(table_path, sample_fraction=0.1):
    """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
    df = spark.read.format("delta").load(table_path)
    sampled_df = df.sample(fraction=sample_fraction, seed=42)
    return sampled_df.toPandas()

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã§ã‚°ãƒ©ãƒ•ä½œæˆ
sampled_data = get_sampled_data("/path/to/large/table", 0.05)

chart = ChartComponent(
    data=sampled_data,
    chart_type='scatter',
    x_column='sales',
    y_column='profit',
    title='ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸå£²ä¸Šãƒ»åˆ©ç›Šãƒ‡ãƒ¼ã‚¿'
)

displayHTML(chart.render())
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯¾å‡¦

```python
# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’åˆ¶é™
def limit_data_size(df, max_rows=10000):
    """ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’åˆ¶é™"""
    if len(df) > max_rows:
        return df.sample(n=max_rows, random_state=42)
    return df

# åˆ¶é™ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ã‚°ãƒ©ãƒ•ä½œæˆ
limited_data = limit_data_size(df, 5000)

chart = ChartComponent(
    data=limited_data,
    chart_type='line',
    x_column='date',
    y_column='sales',
    title='åˆ¶é™ã•ã‚ŒãŸå£²ä¸Šãƒ‡ãƒ¼ã‚¿'
)

displayHTML(chart.render())
```

### 2. ãƒ‡ãƒ¼ã‚¿å‹ã®å•é¡Œ

```python
# ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèªã¨ä¿®æ­£
def fix_data_types(df):
    """ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä¿®æ­£"""
    # æ—¥ä»˜åˆ—ã®ä¿®æ­£
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'])
    
    # æ•°å€¤åˆ—ã®ä¿®æ­£
    numeric_columns = ['sales', 'profit']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    return df

# ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰ã‚°ãƒ©ãƒ•ä½œæˆ
fixed_data = fix_data_types(df)

chart = ChartComponent(
    data=fixed_data,
    chart_type='line',
    x_column='date',
    y_column='sales',
    title='ãƒ‡ãƒ¼ã‚¿å‹ä¿®æ­£æ¸ˆã¿å£²ä¸Šãƒ‡ãƒ¼ã‚¿'
)

displayHTML(chart.render())
```

## ğŸ¯ å®Ÿè·µçš„ãªä¾‹

### å®Œå…¨ãªDatabricksåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```python
# åŒ…æ‹¬çš„ãªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
def create_comprehensive_dashboard():
    """åŒ…æ‹¬çš„ãªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆ"""
    
    # 1. å£²ä¸Šã‚µãƒãƒªãƒ¼
    sales_summary = spark.sql("""
        SELECT 
            DATE_TRUNC('month', date) as month,
            SUM(sales) as total_sales,
            SUM(profit) as total_profit,
            COUNT(*) as transaction_count
        FROM sales_table 
        WHERE date >= '2024-01-01'
        GROUP BY DATE_TRUNC('month', date)
        ORDER BY month
    """).toPandas()
    
    # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    category_analysis = spark.sql("""
        SELECT 
            category,
            SUM(sales) as total_sales,
            AVG(sales) as avg_sales,
            COUNT(*) as transaction_count
        FROM sales_table 
        WHERE date >= '2024-01-01'
        GROUP BY category
        ORDER BY total_sales DESC
    """).toPandas()
    
    # 3. åœ°åŸŸåˆ¥åˆ†æ
    region_analysis = spark.sql("""
        SELECT 
            region,
            SUM(sales) as total_sales,
            SUM(profit) as total_profit
        FROM sales_table 
        WHERE date >= '2024-01-01'
        GROUP BY region
        ORDER BY total_sales DESC
    """).toPandas()
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ
    dashboard = Dashboard(title='DatabricksåŒ…æ‹¬çš„åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰')
    
    # æœˆæ¬¡å£²ä¸Šæ¨ç§»
    monthly_chart = ChartComponent(
        data=sales_summary,
        chart_type='line',
        x_column='month',
        y_column='total_sales',
        title='æœˆæ¬¡å£²ä¸Šæ¨ç§»'
    )
    dashboard.add_component(monthly_chart, position=(0, 0))
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Š
    category_chart = ChartComponent(
        data=category_analysis,
        chart_type='bar',
        x_column='category',
        y_column='total_sales',
        title='ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Š'
    )
    dashboard.add_component(category_chart, position=(0, 1))
    
    # åœ°åŸŸåˆ¥å£²ä¸Š
    region_chart = ChartComponent(
        data=region_analysis,
        chart_type='pie',
        x_column='region',
        y_column='total_sales',
        title='åœ°åŸŸåˆ¥å£²ä¸Šæ§‹æˆ'
    )
    dashboard.add_component(region_chart, position=(1, 0))
    
    # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
    detail_table = TableComponent(
        data=sales_summary,
        enable_csv_download=True,
        sortable=True,
        searchable=True,
        title='æœˆæ¬¡å£²ä¸Šè©³ç´°'
    )
    dashboard.add_component(detail_table, position=(1, 1))
    
    return dashboard

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆã¨è¡¨ç¤º
dashboard = create_comprehensive_dashboard()
displayHTML(dashboard.render())
```

## ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](./performance.md) - Databricksã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
- [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£](./security.md) - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](../troubleshooting/faq.md) - ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

## â“ ã‚µãƒãƒ¼ãƒˆ

Databricksã§ã®ä½¿ç”¨ã§å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ï¼š

1. **ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®š**ã®ç¢ºèª
2. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**ã®ç›£è¦–
3. **ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**ã®ç¢ºèª
4. **ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°**ã®ç¢ºèª

**é–¢é€£ãƒªãƒ³ã‚¯:**
- [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰](./installation.md) - è©³ç´°ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †
- [ã‚ˆãã‚ã‚‹å•é¡Œ](../troubleshooting/faq.md) - Databricksé–¢é€£ã®FAQ